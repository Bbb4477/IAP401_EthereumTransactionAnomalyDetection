{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8500769,"sourceType":"datasetVersion","datasetId":5073272},{"sourceId":11240610,"sourceType":"datasetVersion","datasetId":7022778}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nsecure_df = pd.read_csv(\"/kaggle/input/bccc-vulscs-2023/BCCC-VolSCs-2023_Secure.csv\")\nvulnerable_df = pd.read_csv(\"/kaggle/input/bccc-vulscs-2023/BCCC-VolSCs-2023_Vulnerable.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:05:38.585699Z","iopub.execute_input":"2025-04-01T13:05:38.585989Z","iopub.status.idle":"2025-04-01T13:05:39.567976Z","shell.execute_reply.started":"2025-04-01T13:05:38.585968Z","shell.execute_reply":"2025-04-01T13:05:39.567286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.concat([secure_df, vulnerable_df], ignore_index=True)\nprint(len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:05:57.54075Z","iopub.execute_input":"2025-04-01T13:05:57.541029Z","iopub.status.idle":"2025-04-01T13:05:57.569439Z","shell.execute_reply.started":"2025-04-01T13:05:57.541009Z","shell.execute_reply":"2025-04-01T13:05:57.568562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"contract_codes = []\nfor hash_id in df['hash_id']:\n    file_path = f\"/kaggle/input/contractcodes/source/{hash_id}.sol\"  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        contract_codes.append(file.read())\n\ndf['contract_code'] = contract_codes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:17:24.406775Z","iopub.execute_input":"2025-04-01T13:17:24.407077Z","iopub.status.idle":"2025-04-01T13:17:45.030445Z","shell.execute_reply.started":"2025-04-01T13:17:24.407053Z","shell.execute_reply":"2025-04-01T13:17:45.029581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:18:03.797422Z","iopub.execute_input":"2025-04-01T13:18:03.797764Z","iopub.status.idle":"2025-04-01T13:18:03.823201Z","shell.execute_reply.started":"2025-04-01T13:18:03.797733Z","shell.execute_reply":"2025-04-01T13:18:03.822359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:18:11.10701Z","iopub.execute_input":"2025-04-01T13:18:11.107307Z","iopub.status.idle":"2025-04-01T13:18:14.342596Z","shell.execute_reply.started":"2025-04-01T13:18:11.107285Z","shell.execute_reply":"2025-04-01T13:18:14.341689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel\nimport torch\n\ntokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\nmodel = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n\ndef get_codebert_embedding(code):\n    inputs = tokenizer(code, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  \n\ndf['code_embedding'] = df['contract_code'].apply(get_codebert_embedding)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:18:30.666949Z","iopub.execute_input":"2025-04-01T13:18:30.667385Z","iopub.status.idle":"2025-04-01T13:20:36.816475Z","shell.execute_reply.started":"2025-04-01T13:18:30.667362Z","shell.execute_reply":"2025-04-01T13:20:36.81494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:20:40.886141Z","iopub.execute_input":"2025-04-01T13:20:40.886447Z","iopub.status.idle":"2025-04-01T13:20:40.94625Z","shell.execute_reply.started":"2025-04-01T13:20:40.886422Z","shell.execute_reply":"2025-04-01T13:20:40.945432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should be True\nprint(torch.cuda.device_count())  # Should be >0\nprint(torch.cuda.get_device_name(0))  # Should print \"Tesla P100\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:21:34.426407Z","iopub.execute_input":"2025-04-01T13:21:34.426746Z","iopub.status.idle":"2025-04-01T13:21:34.45811Z","shell.execute_reply.started":"2025-04-01T13:21:34.426719Z","shell.execute_reply":"2025-04-01T13:21:34.457387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\nmodel = RobertaModel.from_pretrained(\"microsoft/codebert-base\").to(device)  # Move model to GPU\n\ndef get_codebert_embedding(code):\n    inputs = tokenizer(code, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\").to(device)  # Move inputs to GPU\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move output back to CPU\n\ndf['code_embedding'] = df['contract_code'].apply(get_codebert_embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:21:47.737087Z","iopub.execute_input":"2025-04-01T13:21:47.737385Z","iopub.status.idle":"2025-04-01T13:28:47.247647Z","shell.execute_reply.started":"2025-04-01T13:21:47.737362Z","shell.execute_reply":"2025-04-01T13:28:47.246265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load tokenizer and model (Optimized)\ntokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", use_fast=True)\nmodel = RobertaModel.from_pretrained(\"microsoft/codebert-base\").to(device)\nmodel.eval()  # Set to evaluation mode\n\n# Set batch size (Optimized for P100 GPU)\nBATCH_SIZE = 32  # Increase if GPU memory allows\n\ndef batch_get_codebert_embedding(texts):\n    \"\"\"Processes a batch of contract codes into embeddings.\"\"\"\n    inputs = tokenizer(texts, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\").to(device)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n\n# Process in batches\nall_embeddings = []\ncontracts = df['contract_code'].tolist()  # Convert DataFrame column to list\n\nfor i in tqdm(range(0, len(contracts), BATCH_SIZE), desc=\"Processing Batches\"):\n    batch = contracts[i:i + BATCH_SIZE]  # Get batch\n    batch_embeddings = batch_get_codebert_embedding(batch)  # Compute embeddings\n    all_embeddings.extend(batch_embeddings)  # Store results\n\n# Add embeddings to DataFrame\ndf['code_embedding'] = list(all_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:28:55.396341Z","iopub.execute_input":"2025-04-01T13:28:55.396701Z","iopub.status.idle":"2025-04-01T13:50:09.459295Z","shell.execute_reply.started":"2025-04-01T13:28:55.396673Z","shell.execute_reply":"2025-04-01T13:50:09.458357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(f\"Shape of X: {X.shape}\")  # Should be (num_samples, embedding_dim + bytecode_features)\n# print(f\"Shape of y: {y.shape}\")  # Should be (num_samples,)\n# print(f\"Sample y values: {np.unique(y)}\")  # Should show [0,1]\n\nprint(df.head())  # Check if 'code_embedding' exists\nprint(df['code_embedding'].dtype)  # Check its data type\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:53:20.716403Z","iopub.execute_input":"2025-04-01T13:53:20.716732Z","iopub.status.idle":"2025-04-01T13:53:20.734289Z","shell.execute_reply.started":"2025-04-01T13:53:20.716708Z","shell.execute_reply":"2025-04-01T13:53:20.733367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df['code_embedding'].head())  \nprint(type(df['code_embedding'].iloc[0]))  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:55:16.801319Z","iopub.execute_input":"2025-04-01T13:55:16.801664Z","iopub.status.idle":"2025-04-01T13:55:16.809271Z","shell.execute_reply.started":"2025-04-01T13:55:16.801639Z","shell.execute_reply":"2025-04-01T13:55:16.808518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nbytecode_features = df[['Weight bytecode_character_6', 'Weight bytecode_character_0', \n                         'Weight bytecode_character_8', 'Weight bytecode_character_4', \n                         'Weight bytecode_character_5', 'Weight bytecode_character_2']]\n\nscaler = StandardScaler()\nbytecode_features = scaler.fit_transform(bytecode_features)\n\nX = np.hstack([np.stack(df['code_embedding'].values), bytecode_features])\ny = df['label'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:55:56.087826Z","iopub.execute_input":"2025-04-01T13:55:56.088115Z","iopub.status.idle":"2025-04-01T13:55:56.223966Z","shell.execute_reply.started":"2025-04-01T13:55:56.088094Z","shell.execute_reply":"2025-04-01T13:55:56.223256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape of X: {X.shape}\")  # Should be (num_samples, embedding_dim + bytecode_features)\nprint(f\"Shape of y: {y.shape}\")  # Should be (num_samples,)\nprint(f\"Sample y values: {np.unique(y)}\")  # Should show [0,1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:56:36.110425Z","iopub.execute_input":"2025-04-01T13:56:36.110783Z","iopub.status.idle":"2025-04-01T13:56:36.117371Z","shell.execute_reply.started":"2025-04-01T13:56:36.110756Z","shell.execute_reply":"2025-04-01T13:56:36.11653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:56:56.290671Z","iopub.execute_input":"2025-04-01T13:56:56.29096Z","iopub.status.idle":"2025-04-01T13:56:56.37425Z","shell.execute_reply.started":"2025-04-01T13:56:56.290939Z","shell.execute_reply":"2025-04-01T13:56:56.373577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_torch = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\nX_val_torch = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_torch = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:04:07.766002Z","iopub.execute_input":"2025-04-01T14:04:07.766311Z","iopub.status.idle":"2025-04-01T14:04:07.918077Z","shell.execute_reply.started":"2025-04-01T14:04:07.766289Z","shell.execute_reply":"2025-04-01T14:04:07.917301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\nclass CodeBERTClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(CodeBERTClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 1)  \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.sigmoid(self.fc3(x))\n        return x\n\nmodel = CodeBERTClassifier(input_dim=X_train.shape[1])\n\ncriterion = nn.BCELoss()  \noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nX_train_torch = torch.tensor(X_train, dtype=torch.float32)\ny_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n\nfor epoch in range(10):  \n    optimizer.zero_grad()\n    outputs = model(X_train_torch)\n    loss = criterion(outputs, y_train_torch)\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass CodeBERTClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(CodeBERTClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 1)  \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.sigmoid(self.fc3(x))\n        return x\n\nmodel = CodeBERTClassifier(input_dim=X_train.shape[1]).to(device)\n\ncriterion = nn.BCELoss()  \noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nX_train_torch = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n\nfor epoch in range(100):  # 10 epochs\n    optimizer.zero_grad()\n    outputs = model(X_train_torch)  # Forward pass\n    loss = criterion(outputs, y_train_torch)  # Compute loss\n    loss.backward()  # Backpropagation\n    optimizer.step()  # Update weights\n    \n    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:59:32.236673Z","iopub.execute_input":"2025-04-01T13:59:32.236999Z","iopub.status.idle":"2025-04-01T13:59:33.129516Z","shell.execute_reply.started":"2025-04-01T13:59:32.236974Z","shell.execute_reply":"2025-04-01T13:59:33.128656Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass CodeBERTClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(CodeBERTClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 2048)\n        self.bn1 = nn.BatchNorm1d(2048)\n        self.dropout1 = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(2048, 1024)\n        self.bn2 = nn.BatchNorm1d(1024)\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(1024, 512)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.dropout3 = nn.Dropout(0.3)\n        self.fc4 = nn.Linear(512, 256)\n        self.bn4 = nn.BatchNorm1d(256)\n        self.dropout4 = nn.Dropout(0.3)\n        self.fc5 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = self.dropout1(self.bn1(torch.relu(self.fc1(x))))\n        x = self.dropout2(self.bn2(torch.relu(self.fc2(x))))\n        x = self.dropout3(self.bn3(torch.relu(self.fc3(x))))\n        x = self.dropout4(self.bn4(torch.relu(self.fc4(x))))\n        x = self.fc5(x)\n        return x\n\nmodel = CodeBERTClassifier(input_dim=X_train.shape[1]).to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.001)\n\nX_train_torch = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\nX_val_torch = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_torch = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n\nfor epoch in range(250):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train_torch)\n    loss = criterion(outputs, y_train_torch)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n\n    model.eval()\n    with torch.no_grad():\n        val_outputs = model(X_val_torch)\n        val_loss = criterion(val_outputs, y_val_torch)\n\n    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:17:47.571062Z","iopub.execute_input":"2025-04-01T14:17:47.571359Z","iopub.status.idle":"2025-04-01T14:18:16.353583Z","shell.execute_reply.started":"2025-04-01T14:17:47.571336Z","shell.execute_reply":"2025-04-01T14:18:16.352818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After training, calculate accuracy on validation set\n\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    val_outputs = model(X_val_torch)\n    # Apply sigmoid to get probabilities\n    val_preds = torch.sigmoid(val_outputs)\n    # Convert probabilities to binary predictions (0 or 1)\n    val_preds = (val_preds > 0.5).float()  # 1 if >0.5, else 0\n\n    # Calculate accuracy: compare predictions with actual labels\n    correct = (val_preds == y_val_torch).sum().item()\n    accuracy = correct / len(y_val_torch)\n\nprint(f\"Validation Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:18:20.672719Z","iopub.execute_input":"2025-04-01T14:18:20.673012Z","iopub.status.idle":"2025-04-01T14:18:20.688759Z","shell.execute_reply.started":"2025-04-01T14:18:20.672991Z","shell.execute_reply":"2025-04-01T14:18:20.688001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n# Get model predictions (0 or 1)\ny_pred = (model(X_val_torch) > 0).cpu().numpy()\n\nprecision = precision_score(y_val, y_pred)\nrecall = recall_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\n\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1-Score: {f1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:19:24.942307Z","iopub.execute_input":"2025-04-01T14:19:24.942624Z","iopub.status.idle":"2025-04-01T14:19:24.973255Z","shell.execute_reply.started":"2025-04-01T14:19:24.942602Z","shell.execute_reply":"2025-04-01T14:19:24.97256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_pred = (model(X_val_torch) > 0).cpu().numpy()\ncm = confusion_matrix(y_val, y_pred)\nprint(f\"Confusion Matrix:\\n{cm}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:19:43.466393Z","iopub.execute_input":"2025-04-01T14:19:43.466749Z","iopub.status.idle":"2025-04-01T14:19:43.484814Z","shell.execute_reply.started":"2025-04-01T14:19:43.466725Z","shell.execute_reply":"2025-04-01T14:19:43.483919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\n# Get the predicted probabilities for class 1 (positive class)\ny_prob = torch.sigmoid(model(X_val_torch)).cpu().detach().numpy()\nfpr, tpr, _ = roc_curve(y_val, y_prob)\nauc_score = auc(fpr, tpr)\n\nprint(f\"AUC: {auc_score}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:20:42.515398Z","iopub.execute_input":"2025-04-01T14:20:42.51572Z","iopub.status.idle":"2025-04-01T14:20:42.533164Z","shell.execute_reply.started":"2025-04-01T14:20:42.515695Z","shell.execute_reply":"2025-04-01T14:20:42.532538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\n\nprob_true, prob_pred = calibration_curve(y_val, y_prob, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label=\"Calibration Curve\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly calibrated\")\nplt.xlabel(\"Mean predicted value\")\nplt.ylabel(\"Fraction of positives\")\nplt.title(\"Calibration Curve\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:21:30.551154Z","iopub.execute_input":"2025-04-01T14:21:30.551439Z","iopub.status.idle":"2025-04-01T14:21:30.868681Z","shell.execute_reply.started":"2025-04-01T14:21:30.551417Z","shell.execute_reply":"2025-04-01T14:21:30.867795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n        return focal_loss.mean()\n\nclass TemperatureScaling(nn.Module):\n    def __init__(self):\n        super(TemperatureScaling, self).__init__()\n        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n        \n    def forward(self, logits):\n        return logits / self.temperature\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features)\n        self.dropout = nn.Dropout(0.3)\n        self.adapter = nn.Linear(in_features, out_features)\n        \n    def forward(self, x):\n        identity = x\n        out = torch.relu(self.fc(x))\n        out = self.bn(out)\n        out = self.dropout(out)\n        return out + self.adapter(identity)\n\nclass ImprovedCodeBERTClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(ImprovedCodeBERTClassifier, self).__init__()\n        # Initial layer\n        self.fc1 = nn.Linear(input_dim, 2048)\n        self.bn1 = nn.BatchNorm1d(2048)\n        self.dropout1 = nn.Dropout(0.3)\n        \n        # Residual blocks\n        self.res1 = ResidualBlock(2048, 1024)\n        self.res2 = ResidualBlock(1024, 512)\n        self.res3 = ResidualBlock(512, 256)\n        \n        # Output layer\n        self.fc_out = nn.Linear(256, 1)\n        \n        # Temperature scaling for calibration\n        self.temperature = TemperatureScaling()\n        \n    def forward(self, x):\n        # Initial layer\n        x = torch.relu(self.fc1(x))\n        x = self.bn1(x)\n        x = self.dropout1(x)\n        \n        # Residual blocks\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.res3(x)\n        \n        # Output layer\n        x = self.fc_out(x)\n        \n        # Apply temperature scaling\n        x = self.temperature(x)\n        \n        return x\n\ndef get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n    \n    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n# Initialize model, loss function, optimizer\nmodel = ImprovedCodeBERTClassifier(input_dim=X_train.shape[1]).to(device)\ncriterion = FocalLoss(alpha=0.25, gamma=2.0)\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-5)\n\n# Learning rate scheduler with warmup\ntotal_epochs = 350  # Increased from 250 to give more training time\nwarmup_steps = 15\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_epochs)\n\n# Prepare data\nX_train_torch = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\nX_val_torch = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_torch = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n\n# Training loop with early stopping\nbest_val_loss = float('inf')\npatience = 20\npatience_counter = 0\nbest_model_state = None\n\nfor epoch in range(total_epochs):\n    # Training\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train_torch)\n    loss = criterion(outputs, y_train_torch)\n    loss.backward()\n    \n    # Gradient clipping\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    \n    optimizer.step()\n    scheduler.step()\n    \n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_outputs = model(X_val_torch)\n        val_loss = criterion(val_outputs, y_val_torch)\n        val_preds = torch.sigmoid(val_outputs) > 0.5\n        val_accuracy = (val_preds == y_val_torch).float().mean().item()\n    \n    print(f\"Epoch {epoch+1}/{total_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}\")\n    \n    # Early stopping check\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        best_model_state = model.state_dict().copy()\n        print(f\"New best model saved! Validation Loss: {best_val_loss:.4f}\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n\n# Load best model\nif best_model_state is not None:\n    model.load_state_dict(best_model_state)\n    print(\"Loaded best model based on validation loss\")\n\n# Final evaluation\nmodel.eval()\nwith torch.no_grad():\n    train_outputs = model(X_train_torch)\n    train_preds = torch.sigmoid(train_outputs) > 0.5\n    train_accuracy = (train_preds == y_train_torch).float().mean().item()\n    \n    val_outputs = model(X_val_torch)\n    val_preds = torch.sigmoid(val_outputs) > 0.5\n    val_accuracy = (val_preds == y_val_torch).float().mean().item()\n    \n    # For calibration curve\n    val_probs = torch.sigmoid(val_outputs).cpu().numpy()\n\nprint(f\"\\nFinal Results:\")\nprint(f\"Training Accuracy: {train_accuracy:.4f}\")\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\n\n# Code to plot new calibration curve\nimport matplotlib.pyplot as plt\nfrom sklearn.calibration import calibration_curve\n\ndef plot_calibration_curve(y_true, y_prob):\n    plt.figure(figsize=(8, 8))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n    \n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    \n    fraction_of_positives, mean_predicted_value = calibration_curve(\n        y_true, y_prob, n_bins=10\n    )\n    \n    ax1.plot(\n        mean_predicted_value,\n        fraction_of_positives,\n        \"s-\",\n        label=f\"Model (Accuracy: {val_accuracy:.3f})\"\n    )\n    \n    ax1.set_ylabel(\"Fraction of positives\")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.set_title(\"Calibration Curve\")\n    ax1.legend(loc=\"lower right\")\n    \n    ax2.hist(y_prob, range=(0, 1), bins=10, histtype=\"step\", lw=2)\n    ax2.set_xlabel(\"Mean predicted value\")\n    ax2.set_ylabel(\"Count\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot the calibration curve for the validation set\nplot_calibration_curve(y_val.ravel(), val_probs.ravel())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:27:12.311434Z","iopub.execute_input":"2025-04-01T14:27:12.311788Z","iopub.status.idle":"2025-04-01T14:27:32.074829Z","shell.execute_reply.started":"2025-04-01T14:27:12.311764Z","shell.execute_reply":"2025-04-01T14:27:32.073912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.calibration import calibration_curve\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Feature dimensionality reduction\ndef reduce_dimensions(X_train, X_val, n_components=150):\n    print(f\"Original feature dimensions: {X_train.shape[1]}\")\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Apply PCA\n    pca = PCA(n_components=n_components, random_state=42)\n    X_train_pca = pca.fit_transform(X_train_scaled)\n    X_val_pca = pca.transform(X_val_scaled)\n    \n    explained_var = np.sum(pca.explained_variance_ratio_)\n    print(f\"Reduced to {n_components} components, explaining {explained_var:.4f} of variance\")\n    \n    return X_train_pca, X_val_pca, pca, scaler\n\n# Advanced regularization: Mixup\ndef mixup_data(x, y, alpha=0.2):\n    '''Generate mixup samples and targets'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(device)\n\n    mixed_x = lam * x + (1 - lam) * x[index]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n# Base model with increased regularization\nclass RegularizedModel(nn.Module):\n    def __init__(self, input_dim, hidden_dims=[512, 256, 128], dropout_rate=0.5):\n        super(RegularizedModel, self).__init__()\n        \n        self.layers = nn.ModuleList()\n        \n        # Input layer\n        self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n        self.layers.append(nn.BatchNorm1d(hidden_dims[0]))\n        self.layers.append(nn.ReLU())\n        self.layers.append(nn.Dropout(dropout_rate))\n        \n        # Hidden layers\n        for i in range(len(hidden_dims)-1):\n            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n            self.layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n            self.layers.append(nn.ReLU())\n            self.layers.append(nn.Dropout(dropout_rate))\n        \n        # Output layer\n        self.output = nn.Linear(hidden_dims[-1], 1)\n        \n        # L2 regularization applied at forward pass\n        self.l2_reg = 1e-4\n        \n    def forward(self, x):\n        # Forward pass through layers\n        for layer in self.layers:\n            x = layer(x)\n        \n        # Output layer\n        x = self.output(x)\n        \n        # L2 regularization\n        l2_reg = 0.0\n        for param in self.parameters():\n            l2_reg += torch.norm(param)\n        \n        # Store for access during training\n        self.l2_loss = self.l2_reg * l2_reg\n        \n        return x\n    \n    def get_l2_loss(self):\n        return self.l2_loss\n\n# Different model architectures for ensemble\nclass WideModel(RegularizedModel):\n    def __init__(self, input_dim):\n        super(WideModel, self).__init__(input_dim, hidden_dims=[1024, 512, 256], dropout_rate=0.5)\n\nclass DeepModel(RegularizedModel):\n    def __init__(self, input_dim):\n        super(DeepModel, self).__init__(input_dim, hidden_dims=[512, 256, 128, 64], dropout_rate=0.4)\n\nclass CompactModel(RegularizedModel):\n    def __init__(self, input_dim):\n        super(CompactModel, self).__init__(input_dim, hidden_dims=[256, 128], dropout_rate=0.3)\n\n# Ensemble model wrapper\nclass EnsembleModel:\n    def __init__(self, models):\n        self.models = models\n        \n    def predict(self, x):\n        predictions = []\n        for model in self.models:\n            model.eval()\n            with torch.no_grad():\n                pred = torch.sigmoid(model(x))\n                predictions.append(pred)\n        \n        # Average predictions\n        ensemble_pred = torch.stack(predictions).mean(dim=0)\n        return ensemble_pred\n\n# Training function with SWA and Mixup\ndef train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, \n                scheduler=None, epochs=200, batch_size=128, \n                patience=25, use_mixup=True, alpha=0.2):\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n    \n    # Convert data to PyTorch tensors if not already\n    if not isinstance(X_train, torch.Tensor):\n        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n    if not isinstance(y_train, torch.Tensor):\n        y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n    if not isinstance(X_val, torch.Tensor):\n        X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n    if not isinstance(y_val, torch.Tensor):\n        y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n    \n    # For SWA (Stochastic Weight Averaging)\n    swa_start = epochs // 2\n    swa_model = torch.optim.swa_utils.AveragedModel(model)\n    swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, swa_lr=0.0005)\n    \n    train_losses = []\n    val_losses = []\n    val_accuracies = []\n    \n    # Calculate number of batches\n    num_train_samples = X_train.shape[0]\n    num_batches = (num_train_samples + batch_size - 1) // batch_size\n    \n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0.0\n        \n        # Create random permutation of indices for batching\n        indices = torch.randperm(num_train_samples).to(device)\n        \n        for i in range(num_batches):\n            # Get batch indices\n            start_idx = i * batch_size\n            end_idx = min((i + 1) * batch_size, num_train_samples)\n            batch_indices = indices[start_idx:end_idx]\n            \n            # Extract batch data\n            batch_x = X_train[batch_indices]\n            batch_y = y_train[batch_indices]\n            \n            optimizer.zero_grad()\n            \n            # Apply mixup if enabled\n            if use_mixup and epoch < epochs * 0.8:  # Use mixup for first 80% of training\n                batch_x, targets_a, targets_b, lam = mixup_data(batch_x, batch_y, alpha)\n                outputs = model(batch_x)\n                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n            else:\n                outputs = model(batch_x)\n                loss = criterion(outputs, batch_y)\n            \n            # Add L2 regularization loss if available\n            if hasattr(model, 'get_l2_loss'):\n                loss += model.get_l2_loss()\n            \n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        # Average loss over batches\n        epoch_loss /= num_batches\n        train_losses.append(epoch_loss)\n        \n        # Update SWA if we're past the start point\n        if epoch >= swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n        elif scheduler is not None:\n            scheduler.step()\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val)\n            val_loss = criterion(val_outputs, y_val).item()\n            val_losses.append(val_loss)\n            \n            val_preds = (torch.sigmoid(val_outputs) > 0.5).float()\n            val_accuracy = (val_preds == y_val).float().mean().item()\n            val_accuracies.append(val_accuracy)\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n        \n        # Early stopping check\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            best_model_state = model.state_dict().copy()\n            print(f\"New best model saved! Validation Loss: {best_val_loss:.4f}\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\n                break\n    \n    # Load the best model state\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n        print(\"Loaded best model based on validation loss\")\n    \n    # If we used SWA, finalize the SWA model\n    if epoch >= swa_start:\n        # Update batch norm statistics for SWA model\n        torch.optim.swa_utils.update_bn(X_train_loader, swa_model)\n        # Evaluate final SWA model\n        swa_model.eval()\n        with torch.no_grad():\n            swa_outputs = swa_model(X_val)\n            swa_loss = criterion(swa_outputs, y_val).item()\n            swa_preds = (torch.sigmoid(swa_outputs) > 0.5).float()\n            swa_accuracy = (swa_preds == y_val).float().mean().item()\n        \n        print(f\"SWA Model - Val Loss: {swa_loss:.4f}, Val Accuracy: {swa_accuracy:.4f}\")\n        \n        # If SWA model is better, use it instead\n        if swa_accuracy > val_accuracies[-1]:\n            print(\"Using SWA model as it performed better\")\n            # Copy SWA parameters to the original model\n            for param_swa, param_model in zip(swa_model.parameters(), model.parameters()):\n                param_model.data.copy_(param_swa.data)\n    \n    return model, train_losses, val_losses, val_accuracies\n\n# Main execution\ndef run_ensemble_training(X_train, y_train, X_val, y_val):\n    # Reduce dimensions\n    X_train_reduced, X_val_reduced, pca, scaler = reduce_dimensions(X_train, X_val, n_components=150)\n    \n    # Convert to tensors\n    X_train_torch = torch.tensor(X_train_reduced, dtype=torch.float32).to(device)\n    y_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n    X_val_torch = torch.tensor(X_val_reduced, dtype=torch.float32).to(device)\n    y_val_torch = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n    \n    # Common training settings\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # Train different models for ensemble\n    input_dim = X_train_reduced.shape[1]\n    models = []\n    \n    # Model 1: Wide architecture\n    print(\"\\n=== Training Wide Model ===\")\n    model1 = WideModel(input_dim).to(device)\n    optimizer1 = optim.AdamW(model1.parameters(), lr=0.001, weight_decay=1e-4)\n    scheduler1 = optim.lr_scheduler.CosineAnnealingLR(optimizer1, T_max=100, eta_min=1e-5)\n    model1, _, _, _ = train_model(model1, X_train_torch, y_train_torch, X_val_torch, y_val_torch, \n                                   criterion, optimizer1, scheduler1, epochs=200, batch_size=128, \n                                   patience=25, use_mixup=True, alpha=0.2)\n    models.append(model1)\n    \n    # Model 2: Deep architecture\n    print(\"\\n=== Training Deep Model ===\")\n    model2 = DeepModel(input_dim).to(device)\n    optimizer2 = optim.AdamW(model2.parameters(), lr=0.002, weight_decay=1e-5)\n    scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='min', factor=0.5, patience=10)\n    model2, _, _, _ = train_model(model2, X_train_torch, y_train_torch, X_val_torch, y_val_torch, \n                                   criterion, optimizer2, scheduler2, epochs=200, batch_size=64, \n                                   patience=25, use_mixup=True, alpha=0.3)\n    models.append(model2)\n    \n    # Model 3: Compact architecture\n    print(\"\\n=== Training Compact Model ===\")\n    model3 = CompactModel(input_dim).to(device)\n    optimizer3 = optim.Adam(model3.parameters(), lr=0.003, weight_decay=1e-6)\n    scheduler3 = optim.lr_scheduler.StepLR(optimizer3, step_size=30, gamma=0.5)\n    model3, _, _, _ = train_model(model3, X_train_torch, y_train_torch, X_val_torch, y_val_torch, \n                                   criterion, optimizer3, scheduler3, epochs=200, batch_size=256, \n                                   patience=25, use_mixup=False)\n    models.append(model3)\n    \n    # Create ensemble\n    ensemble = EnsembleModel(models)\n    \n    # Evaluate individual models\n    train_accuracies = []\n    val_accuracies = []\n    \n    for i, model in enumerate(models):\n        model.eval()\n        with torch.no_grad():\n            # Training accuracy\n            train_outputs = model(X_train_torch)\n            train_preds = (torch.sigmoid(train_outputs) > 0.5).float()\n            train_acc = (train_preds == y_train_torch).float().mean().item()\n            train_accuracies.append(train_acc)\n            \n            # Validation accuracy\n            val_outputs = model(X_val_torch)\n            val_preds = (torch.sigmoid(val_outputs) > 0.5).float()\n            val_acc = (val_preds == y_val_torch).float().mean().item()\n            val_accuracies.append(val_acc)\n            \n        print(f\"Model {i+1} - Train Accuracy: {train_acc:.4f}, Val Accuracy: {val_acc:.4f}\")\n    \n    # Evaluate ensemble\n    ensemble_pred = ensemble.predict(X_train_torch)\n    ensemble_train_preds = (ensemble_pred > 0.5).float()\n    ensemble_train_acc = (ensemble_train_preds == y_train_torch).float().mean().item()\n    \n    ensemble_val_pred = ensemble.predict(X_val_torch)\n    ensemble_val_preds = (ensemble_val_pred > 0.5).float()\n    ensemble_val_acc = (ensemble_val_preds == y_val_torch).float().mean().item()\n    \n    print(f\"\\nEnsemble - Train Accuracy: {ensemble_train_acc:.4f}, Val Accuracy: {ensemble_val_acc:.4f}\")\n    \n    # Generate calibration curve for ensemble\n    val_probs = ensemble_val_pred.cpu().numpy()\n    \n    plot_calibration_curve(y_val.ravel(), val_probs.ravel())\n    \n    return ensemble, models, pca, scaler\n\n# Function to plot calibration curve\ndef plot_calibration_curve(y_true, y_prob):\n    plt.figure(figsize=(8, 8))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n    \n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    \n    fraction_of_positives, mean_predicted_value = calibration_curve(\n        y_true, y_prob, n_bins=10\n    )\n    \n    ax1.plot(\n        mean_predicted_value,\n        fraction_of_positives,\n        \"s-\",\n        label=f\"Ensemble Model\"\n    )\n    \n    ax1.set_ylabel(\"Fraction of positives\")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.set_title(\"Calibration Curve\")\n    ax1.legend(loc=\"lower right\")\n    \n    ax2.hist(y_prob, range=(0, 1), bins=10, histtype=\"step\", lw=2)\n    ax2.set_xlabel(\"Mean predicted value\")\n    ax2.set_ylabel(\"Count\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Execute the ensemble training\nensemble, models, pca, scaler = run_ensemble_training(X_train, y_train, X_val, y_val)\n\n# Final evaluation and summary\nprint(\"\\nFinal Results:\")\nfor i, model in enumerate(models):\n    model.eval()\n    with torch.no_grad():\n        val_outputs = model(torch.tensor(X_val_reduced, dtype=torch.float32).to(device))\n        val_preds = (torch.sigmoid(val_outputs) > 0.5).float()\n        val_acc = (val_preds == torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)).float().mean().item()\n    print(f\"Model {i+1} Validation Accuracy: {val_acc:.4f}\")\n\nensemble_val_pred = ensemble.predict(torch.tensor(X_val_reduced, dtype=torch.float32).to(device))\nensemble_val_preds = (ensemble_val_pred > 0.5).float()\nensemble_val_acc = (ensemble_val_preds == torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)).float().mean().item()\nprint(f\"Ensemble Validation Accuracy: {ensemble_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:31:55.665367Z","iopub.execute_input":"2025-04-01T14:31:55.665707Z","iopub.status.idle":"2025-04-01T14:33:04.723077Z","shell.execute_reply.started":"2025-04-01T14:31:55.665682Z","shell.execute_reply":"2025-04-01T14:33:04.72194Z"}},"outputs":[],"execution_count":null}]}